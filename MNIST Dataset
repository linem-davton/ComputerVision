import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torchvision import transforms
from torch.autograd import Variable
from torch import optim
import numpy as np
import torch.nn.functional as F

from torchvision.datasets import MNIST

dataset = MNIST('data', download=True, transform=transforms.ToTensor())
loader = DataLoader(dataset, batch_size=16, num_workers=4,pin_memory=True)

#Creating A Simple Convolutional Neural Network 
class MNISTNet(nn.Module):
    
    def __init__(self):
        super().__init__()
        self.first_layer = nn.Sequential(nn.Conv2d(1, 16, 3, padding=1), nn.ReLU())
        self.conv_layers = nn.Sequential(*[nn.Sequential(nn.Conv2d(16, 16, 3, padding=1), nn.ReLU()) for _ in range(4)])
        self.last_layer = nn.Linear(28*28*16, 10)
    
    def forward(self, x):
        x = self.first_layer(x)
        x = self.conv_layers(x)
        x = x.reshape(-1, 28*28*16)
        x = self.last_layer(x)
        return x

#Training The Network 
net = MNISTNet()

#Checking For Cuda To Take Advantage of Nvidia Graphics Cards
if torch.cuda.is_available():
    net.cuda()

total_loss = nn.CrossEntropyLoss()
param = net.parameters()
optimizer = optim.Adam(params=param,lr=0.001)


n_epochs=3
n_iterations=0

for e in range(n_epochs):
    for i,(images,labels) in enumerate(loader):
        images = Variable(images)
        labels = Variable(labels)
        if torch.cuda.is_available():
            images = Variable(images).cuda()
            labels = Variable(labels).cuda()
        output = net(images)
        
        net.zero_grad()
        
        loss = total_loss(output,labels)
        loss.backward()
         
        optimizer.step()
        n_iterations+=1

#Saving The Trained Model          
torch.save(net.state_dict(), 'mnist_net.model')
